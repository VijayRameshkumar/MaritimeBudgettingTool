{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2021_2022_2023_expense.csv')\n",
    "\n",
    "df_dd = df[df.CATEGORIES == 'DryDocking Expenses'].reset_index(drop=True)\n",
    "df_pd = df[df['CATEGORIES']=='Pre-Delivery Expenses'].reset_index(drop=True)\n",
    "\n",
    "print(df_dd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_particulars = pd.read_excel('VESSEL_PARTICULARS.xlsx')\n",
    "# vessel_particulars[['VESSEL TYPE', 'VESSEL SUBTYPE']].value_counts().reset_index()['VESSEL SUBTYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segments(dates_series, interval_years=5, interval_months=0):\n",
    "    segments = []\n",
    "    dates_series.sort_values(inplace=True)\n",
    "    current_date = dates_series.iloc[0]  # Start with the minimum date\n",
    "    \n",
    "    for date in dates_series.iloc[1:]:\n",
    "        segment_end = current_date + timedelta(\n",
    "            days=(interval_years * 365.25 + interval_months * 30.44) - 1\n",
    "        )\n",
    "        if date > segment_end:\n",
    "            segments.append((current_date, segment_end))\n",
    "            current_date = date\n",
    "    segments.append((current_date, dates_series.iloc[-1]))  # Last segment\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    dates = pd.to_datetime(x['DATE']).copy()\n",
    "    segments = generate_segments(dates)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation(segments, flag='cat'):\n",
    "    results = []\n",
    "    \n",
    "    if flag == 'subcat':\n",
    "        for x in segments.reset_index(name='segments')[['COST_CENTER', 'segments', 'CATEGORIES', 'ACCOUNT_CODE', 'SUB_CATEGORIES']].values:\n",
    "            for dt in x[1]:\n",
    "                temp = df_dd[df_dd.COST_CENTER == x[0]]\n",
    "                temp['DATE'] = pd.to_datetime(temp['DATE'])\n",
    "                expense = temp[(temp['DATE'] >= x[1][0][0]) & (temp['DATE'] == x[1][0][1])]['AMOUNT_USD'].sum()\n",
    "                results.append((x[0], x[1][0], expense, x[2], x[3], x[4]))\n",
    "    else:\n",
    "        for x in segments.reset_index(name='segments')[['COST_CENTER', 'segments', 'CATEGORIES']].values:\n",
    "            for dt in x[1]:\n",
    "                temp = df_dd[df_dd.COST_CENTER == x[0]]\n",
    "                temp['DATE'] = pd.to_datetime(temp['DATE'])\n",
    "                expense = temp[(temp['DATE'] >= x[1][0][0]) & (temp['DATE'] == x[1][0][1])]['AMOUNT_USD'].sum()\n",
    "                results.append((x[0], x[1][0], expense, x[2]))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_data(df_pd):\n",
    "    cat_df_pd = df_pd.groupby(['COST_CENTER', 'CATEGORIES']).AMOUNT_USD.median()\n",
    "\n",
    "    subcat_df_pd = df_pd.groupby(['COST_CENTER', 'CATEGORIES', 'ACCOUNT_CODE', 'SUB_CATEGORIES']).AMOUNT_USD.median()\n",
    "    \n",
    "    cat_df_pd = cat_df_pd.groupby(['CATEGORIES']).agg(\n",
    "        q1=lambda x: np.quantile(x, 0.25),\n",
    "        q2=lambda x: np.quantile(x, 0.50),\n",
    "        median=lambda x: np.quantile(x, 0.63),\n",
    "        q3=lambda x: np.quantile(x, 0.75)\n",
    "        ).astype(int)\n",
    "    \n",
    "    subcat_df_pd = subcat_df_pd.groupby(['CATEGORIES', 'ACCOUNT_CODE', 'SUB_CATEGORIES']).agg(\n",
    "        q1=lambda x: np.quantile(x, 0.25),\n",
    "        q2=lambda x: np.quantile(x, 0.50),\n",
    "        median=lambda x: np.quantile(x, 0.63),\n",
    "        q3=lambda x: np.quantile(x, 0.75)\n",
    "        ).astype(int)\n",
    "    \n",
    "    return cat_df_pd, subcat_df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dd_cat(DF_DD):\n",
    "    cost_centers = []\n",
    "    expenses = []\n",
    "    \n",
    "    df_dd_cat = DF_DD.groupby(['COST_CENTER', 'PERIOD', 'CATEGORIES']).AMOUNT_USD.sum().reset_index()\n",
    "    df_dd_cat['DATE'] = df_dd_cat['PERIOD'].astype('str').apply(lambda x: f\"{x[:4]}-{x[4:]}-01\" if x else None)\n",
    "    # df_dd_cat.groupby(['COST_CENTER', 'CATEGORIES', 'PERIOD', 'DATE'])['AMOUNT_USD'].sum().reset_index()\n",
    "    cat_seg = df_dd_cat.groupby(['COST_CENTER', 'CATEGORIES']).apply(func)\n",
    "\n",
    "    for rec in cat_seg.reset_index(name='daterange').itertuples():\n",
    "        cc = rec[1]\n",
    "        for dd in rec[3]:\n",
    "            temp = df_dd_cat[(df_dd_cat.COST_CENTER == cc) & (df_dd_cat.DATE >= pd.to_datetime(dd[0]).strftime(\"%Y-%m-%d\")) & (df_dd_cat.DATE <= pd.to_datetime(dd[1]).strftime(\"%Y-%m-%d\"))]\n",
    "            cost_centers.append(cc)\n",
    "            expenses.append(temp.AMOUNT_USD.sum())\n",
    "            \n",
    "    cat_seg_event = pd.DataFrame()\n",
    "    cat_seg_event['COST_CENTER'] = pd.Series(cost_centers)\n",
    "    cat_seg_event['EXPENSE'] = pd.Series(expenses)\n",
    "\n",
    "    filtered_df = cat_seg_event[cat_seg_event.EXPENSE != 0.00]\n",
    "    q1 = filtered_df['EXPENSE'].quantile(0.25)\n",
    "    q2 = filtered_df['EXPENSE'].quantile(0.50)  # This is the median\n",
    "    q3 = filtered_df['EXPENSE'].quantile(0.75)\n",
    "    \n",
    "\n",
    "    # Create a DataFrame with quartile values\n",
    "    return pd.DataFrame({'Quartile': ['CATEGORIES', 'median_50perc_population', 'optimal_63perc_population', 'top_75perc_population'],\n",
    "                                'Value': [rec[2], q1, q2, q3]})\n",
    "    \n",
    "dd_cat = get_dd_cat(df_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dd_subcat(DF_DD):\n",
    "    cost_centers = []\n",
    "    expenses = []\n",
    "    ac_codes=[]\n",
    "    sub_cats = []\n",
    "\n",
    "    df_dd_cat = DF_DD.groupby(['COST_CENTER', 'PERIOD', 'CATEGORIES', 'ACCOUNT_CODE', 'SUB_CATEGORIES']).AMOUNT_USD.sum().reset_index()\n",
    "    df_dd_cat['DATE'] = df_dd_cat['PERIOD'].astype('str').apply(lambda x: f\"{x[:4]}-{x[4:]}-01\" if x else None)\n",
    "    # df_dd_cat.groupby(['COST_CENTER', 'CATEGORIES', 'PERIOD', 'DATE'])['AMOUNT_USD'].sum().reset_index()\n",
    "    cat_seg = df_dd_cat.groupby(['COST_CENTER', 'CATEGORIES', 'ACCOUNT_CODE', 'SUB_CATEGORIES']).apply(func)\n",
    "\n",
    "    for rec in cat_seg.reset_index(name='daterange').itertuples():\n",
    "        cc = rec[1]\n",
    "        for dd in rec[5]:\n",
    "            temp = df_dd_cat[(df_dd_cat.COST_CENTER == cc) & (df_dd_cat.DATE >= pd.to_datetime(dd[0]).strftime(\"%Y-%m-%d\")) & (df_dd_cat.DATE <= pd.to_datetime(dd[1]).strftime(\"%Y-%m-%d\"))]\n",
    "            cost_centers.append(cc)\n",
    "            expenses.append(temp.AMOUNT_USD.sum())\n",
    "            ac_codes.append(rec[3])\n",
    "            sub_cats.append(rec[4])\n",
    "            \n",
    "    subcat_seg_event = pd.DataFrame()\n",
    "    subcat_seg_event['COST_CENTER'] = pd.Series(cost_centers)\n",
    "    subcat_seg_event['CATEGORIES'] = rec[2]\n",
    "    subcat_seg_event['ACCOUNT_CODE'] = pd.Series(ac_codes)\n",
    "    subcat_seg_event['SUB_CATEGORIES'] = pd.Series(sub_cats)\n",
    "    subcat_seg_event['EXPENSE'] = pd.Series(expenses)\n",
    "\n",
    "    filtered_df = subcat_seg_event[subcat_seg_event.EXPENSE != 0.00]\n",
    "\n",
    "    subcat_df_pd = filtered_df.groupby(['CATEGORIES', 'ACCOUNT_CODE', 'SUB_CATEGORIES']).agg(\n",
    "        median_50perc_population=('EXPENSE', lambda x: np.quantile(x, 0.50)),\n",
    "        optimal_63perc_population=('EXPENSE', lambda x: np.quantile(x, 0.63)),\n",
    "        top_75perc_population=('EXPENSE', lambda x: np.quantile(x, 0.75))\n",
    "        ).astype(int)\n",
    "\n",
    "    return subcat_df_pd\n",
    "    \n",
    "dd_subcat = get_dd_subcat(df_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cat = dd_cat.head().set_index('Quartile').T.reset_index(drop=True)\n",
    "dd_subcat = dd_subcat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_cat, pd_subcat =  get_pd_data(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_cat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06/20/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.read_excel('event_excel.xlsx')\n",
    "\n",
    "event_df['median_50perc_population'] = event_df['median_50perc_population'].astype(int)\n",
    "event_df['optimal_63perc_population'] = event_df['optimal_63perc_population'].astype(int)\n",
    "event_df['top_75perc_population'] = event_df['top_75perc_population'].astype(int)\n",
    "\n",
    "grp_tot_cat = event_df.groupby(['CATEGORIES']).sum().reset_index().sum().to_dict()\n",
    "\n",
    "\n",
    "# condition = grp_tot_cat[\"Header\"].isin(['Total OPEX', 'OPEX/DAY'])\n",
    "\n",
    "# if condition.shape[0] > 0:\n",
    "#     grp_tot_cat.loc[condition, 'Stats Model - Optimal Budget'] = None\n",
    "#     grp_tot_cat.loc[condition, 'median_50perc_population'] = None\n",
    "#     grp_tot_cat.loc[condition, 'optimal_63perc_population'] = None\n",
    "#     grp_tot_cat.loc[condition, 'top_75perc_population'] = None\n",
    "\n",
    "# grp_tot_cat['order'] = grp_tot_cat['Header'].apply(lambda x: order.index(x) if x in order else len(order))\n",
    "# grp_tot_cat = grp_tot_cat.sort_values(by='CATEGORIES', ascending=True)\n",
    "# grp_tot_cat = grp_tot_cat.set_index('CATEGORIES')        \n",
    "# grp_tot_cat = grp_tot_cat.sort_values(by='CATEGORIES', ascending=True)\n",
    "\n",
    "# grp_tot_cat = grp_tot_cat.T.to_json()\n",
    "\n",
    "# ## cate vise sum\n",
    "json_data = {}\n",
    "\n",
    "# # Group by 'Header' column\n",
    "grouped = event_df.groupby(['CATEGORIES'])\n",
    "\n",
    "# # Iterate over groups\n",
    "for group_name, group_data in grouped:\n",
    "    group_json = group_data.to_dict(orient='records')\n",
    "    json_data[group_name] = group_json\n",
    "    json_output = json.dumps(json_data, indent=4)\n",
    "\n",
    "# grp_tot_cat = json.loads(grp_tot_cat)\n",
    "# for key in grp_tot_cat.keys():\n",
    "#     grp_tot_cat[key].update({\"records\": json_data[key]})\n",
    "#     grp_tot_cat[key].update({\"Header\": key})                \n",
    "    \n",
    "\n",
    "# data = []\n",
    "# for key, value in grp_tot_cat.items():\n",
    "#     data.append(grp_tot_cat[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df['median_50perc_population'] = event_df['median_50perc_population'].astype(int)\n",
    "event_df['optimal_63perc_population'] = event_df['optimal_63perc_population'].astype(int)\n",
    "event_df['top_75perc_population'] = event_df['top_75perc_population'].astype(int)\n",
    "\n",
    "grp_tot_cat = event_df.groupby(['CATEGORIES']).sum().reset_index().sum().to_dict()\n",
    "grp_tot_cat['CATEGORIES'] = 'EVENT CATEGORIES'\n",
    "grp_tot_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "event_df = pd.read_excel('event_excel.xlsx')\n",
    "\n",
    "event_df['median_50perc_population'] = event_df['median_50perc_population'].astype(int)\n",
    "event_df['optimal_63perc_population'] = event_df['optimal_63perc_population'].astype(int)\n",
    "event_df['top_75perc_population'] = event_df['top_75perc_population'].astype(int)\n",
    "\n",
    "event_df['SUBCATEGORIES'] = event_df['ACCOUNT_CODE'] + \"; \" + event_df['SUB_CATEGORIES']\n",
    "final_json = event_df[['CATEGORIES', 'median_50perc_population', 'optimal_63perc_population', 'top_75perc_population']].groupby('CATEGORIES').sum().T.to_dict()\n",
    "\n",
    "dd_recs = []\n",
    "pd_recs = []\n",
    "\n",
    "grouped = event_df[['CATEGORIES', 'SUBCATEGORIES', 'median_50perc_population', 'optimal_63perc_population', 'top_75perc_population']].rename(columns={'CATEGORIES':'Header'}).groupby('Header')\n",
    "\n",
    "# Iterate over groups and convert each group to a dictionary\n",
    "for group_name, group_data in grouped:\n",
    "    group_dict = {\n",
    "        'CATEGORY': group_name,\n",
    "        'data': group_data.to_dict(orient='records')\n",
    "    }\n",
    "\n",
    "    item_k = list(group_dict.keys())\n",
    "    \n",
    "    if group_dict[item_k[0]] == 'DRYDOCKING EXPENSES':\n",
    "        dd_recs.extend(group_dict[item_k[1]])\n",
    "    elif group_dict[item_k[0]] == 'PRE-DELIVERY EXPENSES':\n",
    "        pd_recs.extend(group_dict[item_k[1]])\n",
    "        \n",
    "final_json['DRYDOCKING EXPENSES'].update({'records': dd_recs})\n",
    "final_json['PRE-DELIVERY EXPENSES'].update({'records' : pd_recs}) \n",
    "recs = list(final_json.values())\n",
    "\n",
    "event_df = event_df.groupby(['CATEGORIES']).sum().reset_index().sum().to_dict()\n",
    "event_df['CATEGORIES'] = 'TOTAL EVENT EXPENSE'\n",
    "event_df['Header'] = 'TOTAL EVENT EXPENSE'\n",
    "\n",
    "recs.append(event_df)\n",
    "\n",
    "with open(\"valid.json\", 'w') as f:\n",
    "    json.dump(recs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "account = os.getenv('account')\n",
    "user = os.getenv('user')\n",
    "password = os.getenv('password')\n",
    "warehouse = os.getenv('warehouse')\n",
    "database = os.getenv('database')\n",
    "schema = os.getenv('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(query, database=None, schema=None):\n",
    "    # Establish the connection\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=user,\n",
    "        password= password,\n",
    "        account= account,\n",
    "        warehouse= warehouse,\n",
    "        database= database,\n",
    "        schema= schema\n",
    "    )\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch and print the result\n",
    "    result = cursor.fetchall()\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expense_data():\n",
    "    unique_cc = pd.read_excel('VESSEL_PARTICULARS.xlsx', engine='openpyxl')['VESSEL CODE'].unique().tolist()\n",
    "    unique_cc_str = ', '.join([f\"'{cc}'\" for cc in unique_cc])\n",
    "    \n",
    "    import datetime\n",
    "    # Get the current year\n",
    "    current_year = datetime.datetime.now().year\n",
    "\n",
    "    # Calculate the start and end periods\n",
    "    start_period = (current_year - 3) * 100 + 1  # January of the year 3 years ago\n",
    "    end_period = (current_year - 1) * 100 + 12  # December of last year        \n",
    "\n",
    "    expense_query = f'''\n",
    "    SELECT\n",
    "        TO_VARIANT(DATE_TRUNC('MONTH', MS.POSTING_DATE))::VARCHAR AS \"Year-Month\",\n",
    "        MS.COST_CENTER,\n",
    "        MS.NODECODE AS \"Categories\",\n",
    "        MS.ACCOUNT_CODE,\n",
    "        MS.ACCOUNT_CODE_DESC AS \"SUB_CATEGORIES\",\n",
    "        SUM(MS.AMOUNT_USD) AS \"Expense\"\n",
    "    FROM\n",
    "        CURATED_DB.VESSEL_ACCOUNTS.MANAGER_STATEMENT MS\n",
    "    WHERE\n",
    "        MS.BRANCHCODE = 'Operating Expenses' AND \n",
    "        MS.COST_CENTER IN ({unique_cc_str}) AND \n",
    "        MS.PERIOD >= {start_period} AND \n",
    "        MS.PERIOD <= {end_period}\n",
    "    GROUP BY \n",
    "        \"Year-Month\", \n",
    "        MS.COST_CENTER, \n",
    "        MS.NODECODE, \n",
    "        MS.ACCOUNT_CODE, \n",
    "        MS.ACCOUNT_CODE_DESC\n",
    "    ORDER BY \n",
    "        \"Year-Month\", \n",
    "        MS.COST_CENTER, \n",
    "        MS.NODECODE ASC\n",
    "    '''\n",
    "\n",
    "    expense_data = pd.DataFrame(get_data(expense_query), columns=['DATE', 'COST_CENTER', 'CATEGORIES', \"ACCOUNT_CODE\", 'SUB_CATEGORIES', 'Expense'])\n",
    "    expense_data['PERIOD'] = pd.to_datetime(expense_data['DATE']).dt.strftime('%Y%m').astype('int')\n",
    "    expense_data = expense_data[(expense_data['PERIOD'] >= start_period) & (expense_data['PERIOD'] <= end_period)]\n",
    "    \n",
    "    return expense_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_expense_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.DATE.min(), data.DATE.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202101 202312\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
